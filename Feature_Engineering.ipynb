{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering _Machine Learning Basics"
      ],
      "metadata": {
        "id": "LcERmJcWsGEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1.What is a parameter?\n",
        "Ans:a Parameter typically refers to a variable or attribute that is used as input for a machine learning model. These parameters are essentially the features that the model uses to make predictions or decisions.\n",
        "\n",
        "####2. What is correlation? What does negative correlation mean?\n",
        "Ans: Correlation measures the relationship between two variables (features). It tells us how one feature's change is associated with another feature's change. Correlation values range from -1 to +1\n",
        "Negative correlation means one feature increases, the other decreases.\n",
        "\n",
        "####3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "Ans: Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to learn from and make predictions or decisions based on data.\n",
        "\n",
        "Major components of ML:\n",
        "\n",
        "Data-Data collected from various sources, Cleaned and transformed data ready for analysis and modeling.\n",
        "\n",
        "Features- The process of selecting, modifying, and creating features that can enhance model performance(Feature Engineering). Identifying the most relevant features that have a significant impact on the target variable(Feature Selection).\n",
        "\n",
        "Model-The mathematical formulation or procedure that the model uses to make predictions or decisions.The process of feeding data into the model and adjusting parameters to minimize error and improve accuracy.\n",
        "\n",
        "Evalution-Criteria used to assess model performance, such as accuracy, precision, and recall.\n",
        "\n",
        "Loss-Function: A mathematical function that measures the difference between the predicted values and the actual values.\n",
        "\n",
        "Optimization- Updating the parameters to minimize the complexity.\n",
        "\n",
        "Deployment- Implementing the trained model into a real-world environment where it can make predictions on new data.\n",
        "\n",
        "####4.How does loss value help in determining whether the model is good or not?\n",
        "A mathematical function that measures the difference between the predicted values and the actual values.The output of the loss function, representing how well or poorly the model is performing. Lower loss values indicate better performance.\n",
        "\n",
        "Overfitting: When a model performs well on training data but poorly on validation data. A very low training loss but high validation loss indicates overfitting.\n",
        "\n",
        "Underfitting: When a model performs poorly on both training and validation data. High loss values for both training and validation sets indicate underfitting.\n",
        "\n",
        "Optimization Process:\n",
        "\n",
        "The optimization algorithm (e.g., gradient descent) uses the loss value to update the model's parameters in the direction that minimizes the loss, thereby improving the model's accuracy.\n",
        "\n",
        "Evaluating Model Performance\n",
        "Training Loss vs. Validation Loss:\n",
        "It's essential to monitor both the training loss and validation loss to assess the model's generalization ability. Consistently low validation loss indicates a model that generalizes well to new, unseen data.\n",
        "\n",
        "Learning Curves:\n",
        "\n",
        "Plotting the loss values helps visualize the model's learning process. A well-performing model typically shows a decreasing trend in both training and validation loss.\n",
        "\n",
        "####5. What are continuous and categorical variables?\n",
        "\n",
        "Ans: Continuous variables are numerical variables that can take an infinite number of values within a given range. They are measured on a continuous scale and can be broken down into smaller parts. Examples include:\n",
        "Height: 165.5 cm, 180.2 cm\n",
        "Weight: 70.4 kg, 65.2 kg\n",
        "Temperature: 22.3°C, 30.7°C\n",
        "Time: 12.45 seconds, 7.89 minutes\n",
        "\n",
        "Continuous variables can take any value, including fractions and decimals, which allows for a wide range of possible values.\n",
        "\n",
        "Categorical variables are variables that represent distinct categories or groups. These variables are qualitative and can be divided into two types:\n",
        "\n",
        "Nominal Variables: Categories without any intrinsic order. Examples:\n",
        "Color: Red, Blue, Green\n",
        "Gender: Male, Female, Non-binary\n",
        "Country: India, USA, Japan\n",
        "\n",
        "Ordinal Variables: Categories with a meaningful order or ranking. Examples:\n",
        "Education Level: High School, Bachelor's, Master's, PhD\n",
        "Rating: Poor, Fair, Good, Excellent\n",
        "Size: Small, Medium, Large\n",
        "\n",
        "####6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Ans: One-Hot Encoding transforms categorical variables into a series of binary columns (0 or 1).\n",
        "\n",
        "Label & Nominal Encoding converts categorical values into numeric values. Each category is assigned a unique integer. This method works well for ordinal variables but may not be suitable for nominal variables due to the implicit ordering.\n",
        "\n",
        "Ordinal Encoding assigns integer values to ordinal categorical variables, maintaining the order of the categories.\n",
        "\n",
        "Target Encoding replaces each category with the mean of the target variable for that category.\n",
        "\n",
        "####7.What do you mean by training and testing a dataset?\n",
        "Ans: Training is the process where the machine learning model learns from the data.Model Training is used to fit the machine learning model. The model learns patterns and relationships within the data by adjusting its parameters to minimize errors.\n",
        "\n",
        "Testing is the process of evaluating the model's performance using unseen data.Model Evaluation: The testing dataset (which the model has never seen during training) is used to evaluate how well the model generalizes to new data.\n",
        "\n",
        "Performance Metrics: Common metrics like accuracy, precision, recall, and F1 score are calculated to assess the model's performance.\n",
        "\n",
        "Validation: This step helps in identifying if the model is overfitting (performing well on training data but poorly on testing data) or underfitting (performing poorly on both training and testing data).\n",
        "\n",
        "####8.What is sklearn.preprocessing?\n",
        "Ans: sklearn.preprocessing is a module in the popular Python machine learning library, scikit-learn. This module includes a variety of tools for preprocessing data, which is a crucial step in preparing data for machine learning models. Preprocessing involves transforming raw data into a format that can be efficiently and effectively used by machine learning algorithms.\n",
        "\n",
        "####9.What is a Test set?\n",
        "\n",
        "Ans:In machine learning, a test set is a subset of the dataset that is used to evaluate the performance of a trained model.The primary purpose of the test set is to assess how well the model generalizes to new, unseen data. This helps ensure that the model isn't just memorizing the training data but can make accurate predictions on real-world data.\n",
        "Using the test set, various performance metrics such as accuracy, precision and recall.\n",
        "\n",
        "####10.How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
        "\n",
        "Ans: To split data into training and testing sets in Python, you can use the train_test_split function from the scikit-learn library.\n",
        "Problem Definition:\n",
        "\n",
        "ML Approach :\n",
        "Clearly define the problem you're trying to solve and the desired outcome.\n",
        "Identify the target variable (the variable you want to predict).\n",
        "\n",
        "Data Collection:\n",
        "Gather raw data from relevant sources, such as databases, APIs, or datasets.\n",
        "Ensure you have enough data to train and test your model effectively.\n",
        "\n",
        "Data Preprocessing:\n",
        "Clean the data: Handle missing values, remove duplicates, and correct errors.\n",
        "Encode categorical variables using techniques like One-Hot Encoding or Label Encoding.Scale or normalize numerical features if necessary.\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "Visualize the data using plots and charts to understand patterns and relationships.Calculate summary statistics to gain insights into the data.\n",
        "\n",
        "Feature Engineering:\n",
        "Select and create relevant features that can improve model performance.\n",
        "Handle feature interactions and transformations if needed.\n",
        "\n",
        "Dataset Splitting:\n",
        "Split the data into training and testing sets using train_test_split.\n",
        "Optionally, create a validation set for hyperparameter tuning and model selection.\n",
        "\n",
        "Model Selection:\n",
        "Choose appropriate machine learning algorithms based on the problem type (regression, classification, clustering, etc.).Consider multiple models and compare their performance.\n",
        "\n",
        "Model Training:\n",
        "Train the selected models using the training data.Optimize the model parameters to minimize the loss function.\n",
        "\n",
        "Model Evaluation:\n",
        "Evaluate the trained models on the test set using relevant metrics (accuracy, precision, recall, F1 score, etc.).Use cross-validation to ensure the model generalizes well to new data.\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "Use techniques like Grid Search or Random Search to find the best hyperparameters for the model.Fine-tune the model to achieve optimal performance.\n",
        "\n",
        "Model Deployment:\n",
        "Deploy the trained model to a production environment where it can make predictions on new data.Monitor model performance and update it as needed.\n",
        "\n",
        "Documentation and Reporting:\n",
        "Document the entire process, including data sources, preprocessing steps, model selection, and evaluation metrics.Create a report or presentation to communicate the findings and results to stakeholders.\n",
        "\n",
        "By following these steps, you can systematically approach and solve a machine learning problem, ensuring that your model is robust, accurate, and ready for real-world applications.\n",
        "\n",
        "####11.Why do we have to perform EDA before fitting a model to the data?\n",
        "Ans:Exploratory Data Analysis (EDA) is like the warm-up before a workout—it prepares your data and reveals insights that guide your next steps.\n",
        "Understand Data Structure: EDA helps you understand the structure, distribution, and relationships within your data. It reveals patterns, anomalies, and outliers, giving you a solid foundation.\n",
        "\n",
        "Detect Errors and Outliers: You can identify and handle missing values, outliers, and errors. Ignoring these can lead to misleading results and poor model performance.\n",
        "\n",
        "Feature Selection: EDA helps in selecting relevant features and discarding irrelevant ones. This step ensures that your model is built on meaningful data.\n",
        "\n",
        "Data Transformation: It often requires transforming data (e.g., normalization, log transformation) to improve model performance.\n",
        "\n",
        "Hypothesis Testing: EDA enables you to formulate hypotheses about your data and test them before building models. This step guides the selection of appropriate algorithms and parameters.\n",
        "\n",
        "Visual Insights: Visualizing data through plots and charts provides intuitive insights that are hard to glean from raw numbers.\n",
        "\n",
        "Preventing Overfitting: By understanding your data, you can avoid overfitting—a common issue where a model performs well on training data but poorly on new data.\n",
        "\n",
        "In essence, EDA is your data’s “getting-to-know-you” phase. It ensures you're making informed decisions and setting your model up for success.\n",
        "\n",
        "####12.What is correlation?\n",
        "Ans: Correlation measures the relationship between two variables, indicating how they move in relation to each other. Corelation has been classifid into 3 ctegories i.e Positive, Negative and no-correlation.\n",
        "\n",
        "####13.What does negative correlation mean?\n",
        "Ans:\n",
        "Negative Correlation: If one variable increases, the other decreases. For example, the number of hours spent watching TV and academic grades might show a negative correlation.\n",
        "\n",
        "\n",
        "####14.How can you find correlation between variables in Python?\n",
        "Ans: correlation = df['var1'].corr(df['var2'])\n",
        "print('Correlation between var1 and var2:', correlation)\n",
        "\n",
        "\n",
        "####15.What is causation? Explain difference between correlation and causation with an example.\n",
        "Ans: Causation, also known as causality, is when one event (the cause) directly affects another event (the effect). In other words, changes in one variable directly result in changes in another variable.\n",
        "Correlation indicates a relationship or association between two variables, but it does not imply that one causes the other.\n",
        "\n",
        "Causation means that one variable directly affects the other.\n",
        "\n",
        "Ex:Smoking and lung cancer have a causal relationship. Numerous scientific studies have shown that smoking increases the risk of developing lung cancer. In this case, smoking (cause) directly leads to an increased likelihood of lung cancer (effect).\n",
        "Ex: Typically if we notice there is a positive correlation between a person's height and their shoe size. As a person grows taller, their feet also tend to get bigger.This is because both height and foot size are influenced by overall body growth and development.\n",
        "This doesn't mean that having bigger feet causes a person to be taller, but rather, they grow together due to shared biological factors.\n",
        "\n",
        "####16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "Ans:Optimizer is an algorithm or method used to adjust the parameters of a model to minimize (or maximize) a loss function. Optimizers are crucial for training models as they ensure the model learns the patterns in the data effectively.\n",
        "Types of Optimizers:\n",
        "#####Gradient Descent (GD)\n",
        "Description: Gradient Descent is a fundamental optimization algorithm that iteratively adjusts model parameters in the direction of the steepest descent of the loss function.\n",
        "Example: Predicting the stock market prices based on historical data such as closing prices, volume, and financial indicators.\n",
        "\n",
        "#####Stochastic Gradient Descent (SGD)\n",
        "Description: Similar to Gradient Descent, but it updates the parameters using only a single or a small batch of training examples at each iteration, making it more efficient for large datasets.\n",
        "Example: Personalizing online shopping experiences by recommending products based on user behavior, such as past purchases and browsing history.\n",
        "\n",
        "#####Mini-Batch Gradient Descent\n",
        "Description: A compromise between GD and SGD, Mini-Batch Gradient Descent updates parameters using a subset of the training data (mini-batch) at each iteration.\n",
        "Example: Imagine a chef preparing multiple dishes for a large party. Instead of cooking one dish at a time (like Stochastic Gradient Descent) or trying to cook all the dishes simultaneously (like Gradient Descent), you decide to prepare a few dishes in small batches.\n",
        "\n",
        "Step-by-Step:\n",
        "You start by chopping vegetables for 3 dishes at a time.\n",
        "Then, you sauté the ingredients for those 3 dishes together.\n",
        "Next, you move on to the next set of 3 dishes, and repeat the process.\n",
        "This approach allows you to manage your time efficiently, making steady progress without feeling overwhelmed.\n",
        "\n",
        "#####Momentum\n",
        "Description: Momentum is an extension of SGD that helps accelerate convergence by adding a fraction of the previous update to the current update.\n",
        "Example: Imagine rolling a ball down a hill; momentum helps the ball gather speed, overcoming small bumps more effectively.\n",
        "\n",
        "#####RMSprop (Root Mean Square Propagation)\n",
        "Description: RMSprop divides the learning rate by a moving average of recent gradient magnitudes, adapting the learning rate based on how quickly parameters are changing.\n",
        "Example: It’s like running down a hill with an adjustable pace, slowing down in steep areas to avoid overshooting.Imagine you're walking down a winding path with different terrains—some smooth, some rocky. RMSprop is like having shoes that automatically adjust their grip depending on the terrain, helping you walk steadily without slipping.\n",
        "\n",
        "#####Adam (Adaptive Moment Estimation)\n",
        "Description: Adam combines the benefits of RMSprop and momentum, maintaining adaptive learning rates for each parameter and accelerating convergence.\n",
        "\n",
        "Example: Think of Adam as a smart GPS that not only guides you to your destination but also adjusts your speed based on traffic conditions. It combines the benefits of adapting to changing conditions (like RMSprop) and maintaining momentum (like the shopping cart example).\n",
        "\n",
        "\n",
        "\n",
        "####17.What is sklearn.linear_model ?\n",
        "Ans: Sklearn.linear_model is a module in the scikit-learn library, which is widely used for machine learning in Python. This module includes a variety of linear models that are commonly used for regression and classification tasks.\n",
        "\n",
        "\n",
        "####18.What does model.fit() do? What arguments must be given?\n",
        "Ans: The model.fit() function in machine learning libraries is used to train the model on the provided training data. During this process, the model learns the patterns in the data and adjusts its parameters to minimize the error (loss) between the predicted and actual outcomes.\n",
        "\n",
        "####19.What does model.predict() do? What arguments must be given?\n",
        "Ans: The model.predict() function in machine learning libraries is used to generate predictions from the trained model. In essence, it takes the input data and applies the learned model parameters to predict the output.\n",
        "\n",
        "####20.What are continuous and categorical variables?\n",
        "Ans: Continuous variables are numerical variables that can take an infinite number of values within a given range. They are measured on a continuous scale and can be broken down into smaller parts. Examples include:\n",
        "Height: 165.5 cm, 180.2 cm\n",
        "Weight: 70.4 kg, 65.2 kg\n",
        "Temperature: 22.3°C, 30.7°C\n",
        "Time: 12.45 seconds, 7.89 minutes\n",
        "\n",
        "Continuous variables can take any value, including fractions and decimals, which allows for a wide range of possible values.\n",
        "\n",
        "Categorical variables are variables that represent distinct categories or groups. These variables are qualitative and can be divided into two types:\n",
        "\n",
        "Nominal Variables: Categories without any intrinsic order. Examples:\n",
        "Color: Red, Blue, Green\n",
        "Gender: Male, Female, Non-binary\n",
        "Country: India, USA, Japan\n",
        "\n",
        "Ordinal Variables: Categories with a meaningful order or ranking. Examples:\n",
        "Education Level: High School, Bachelor's, Master's, PhD\n",
        "Rating: Poor, Fair, Good, Excellent\n",
        "Size: Small, Medium, Large\n",
        "\n",
        "####21.What is feature scaling? How does it help in Machine Learning?\n",
        "Ans: Feature scaling is a technique used to standardize the range of independent variables or features of data. In other words, it helps bring all the features onto a similar scale, which is particularly important when features have different units or ranges.\n",
        "It helps by standardizing features, you ensure that each feature contributes equally to the model's learning process, improving accuracy and convergence rates.\n",
        "\n",
        "####22.How do we perform scaling in Python?\n",
        "Ans: import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "heights = np.array([[150], [160], [165], [170], [180]])\n",
        "scaler = StandardScaler()\n",
        "scaled_heights = scaler.fit_transform(heights)\n",
        "print(scaled_heights)\n",
        "\n",
        "\n",
        "####23.What is sklearn.preprocessing?\n",
        "Ans: sklearn.preprocessing is a module in the scikit-learn library, which provides various utilities and functions for data preprocessing and transformation. These tools are essential for preparing raw data before feeding it into machine learning algorithms, ensuring that the data is in an appropriate format and scale.\n",
        "\n",
        "####24.How do we split data for model fitting (training and testing) in Python?\n",
        "Ans: Ans: To split data into training and testing sets in Python, you can use the train_test_split function from the scikit-learn library.\n",
        "Problem Definition:\n",
        "\n",
        "ML Approach :\n",
        "Clearly define the problem you're trying to solve and the desired outcome.\n",
        "Identify the target variable (the variable you want to predict).\n",
        "\n",
        "Data Collection:\n",
        "Gather raw data from relevant sources, such as databases, APIs, or datasets.\n",
        "Ensure you have enough data to train and test your model effectively.\n",
        "\n",
        "Data Preprocessing:\n",
        "Clean the data: Handle missing values, remove duplicates, and correct errors.\n",
        "Encode categorical variables using techniques like One-Hot Encoding or Label Encoding.Scale or normalize numerical features if necessary.\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "Visualize the data using plots and charts to understand patterns and relationships.Calculate summary statistics to gain insights into the data.\n",
        "\n",
        "Feature Engineering:\n",
        "Select and create relevant features that can improve model performance.\n",
        "Handle feature interactions and transformations if needed.\n",
        "\n",
        "Dataset Splitting:\n",
        "Split the data into training and testing sets using train_test_split.\n",
        "Optionally, create a validation set for hyperparameter tuning and model selection.\n",
        "\n",
        "Model Selection:\n",
        "Choose appropriate machine learning algorithms based on the problem type (regression, classification, clustering, etc.).Consider multiple models and compare their performance.\n",
        "\n",
        "Model Training:\n",
        "Train the selected models using the training data.Optimize the model parameters to minimize the loss function.\n",
        "\n",
        "Model Evaluation:\n",
        "Evaluate the trained models on the test set using relevant metrics (accuracy, precision, recall, F1 score, etc.).Use cross-validation to ensure the model generalizes well to new data.\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "Use techniques like Grid Search or Random Search to find the best hyperparameters for the model.Fine-tune the model to achieve optimal performance.\n",
        "\n",
        "Model Deployment:\n",
        "Deploy the trained model to a production environment where it can make predictions on new data.Monitor model performance and update it as needed.\n",
        "\n",
        "Documentation and Reporting:\n",
        "Document the entire process, including data sources, preprocessing steps, model selection, and evaluation metrics.Create a report or presentation to communicate the findings and results to stakeholders.\n",
        "\n",
        "By following these steps, you can systematically approach and solve a machine learning problem, ensuring that your model is robust, accurate, and ready for real-world applications.\n",
        "\n",
        "####25.Explain data encoding?\n",
        "Ans: Data encoding is the process of transforming categorical data into numerical formats so that machine learning algorithms can process it effectively. Many machine learning models require numerical input, and categorical features need to be converted for proper analysis.\n",
        "Types:\n",
        "One-Hot Encoding\tConverts categories into a binary matrix.\n",
        "\n",
        "Label/Ordinal Encoding:\tAssigns a unique integer to each category. Converts categories into ordered integers\n",
        "\n",
        "Target Encoding\tReplaces categories with mean of the target variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "ek40z1qssWw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWYy2jRssBcM"
      },
      "outputs": [],
      "source": []
    }
  ]
}